{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# Welcome to Week 2!\n",
    "\n",
    "## Frontier Model APIs\n",
    "\n",
    "In Week 1, we used multiple Frontier LLMs through their Chat UI, and we connected with the OpenAI's API.\n",
    "\n",
    "Today we'll connect with the APIs for Anthropic and Google, as well as OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Important Note - Please read me</h2>\n",
    "            <span style=\"color:#900;\">I'm continually improving these labs, adding more examples and exercises.\n",
    "            At the start of each week, it's worth checking you have the latest code.<br/>\n",
    "            First do a <a href=\"https://chatgpt.com/share/6734e705-3270-8012-a074-421661af6ba9\">git pull and merge your changes as needed</a>. Any problems? Try asking ChatGPT to clarify how to merge - or contact me!<br/><br/>\n",
    "            After you've pulled the code, from the llm_engineering directory, in an Anaconda prompt (PC) or Terminal (Mac), run:<br/>\n",
    "            <code>conda env update --f environment.yml</code><br/>\n",
    "            Or if you used virtualenv rather than Anaconda, then run this from your activated environment in a Powershell (PC) or Terminal (Mac):<br/>\n",
    "            <code>pip install -r requirements.txt</code>\n",
    "            <br/>Then restart the kernel (Kernel menu >> Restart Kernel and Clear Outputs Of All Cells) to pick up the changes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Reminder about the resources page</h2>\n",
    "            <span style=\"color:#f71;\">Here's a link to resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## Setting up your keys\n",
    "\n",
    "If you haven't done so already, you could now create API keys for Anthropic and Google in addition to OpenAI.\n",
    "\n",
    "**Please note:** if you'd prefer to avoid extra API costs, feel free to skip setting up Anthopic and Google! You can see me do it, and focus on OpenAI for the course. You could also substitute Anthropic and/or Google for Ollama, using the exercise you did in week 1.\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api  \n",
    "\n",
    "### Also - adding DeepSeek if you wish\n",
    "\n",
    "Optionally, if you'd like to also use DeepSeek, create an account [here](https://platform.deepseek.com/), create a key [here](https://platform.deepseek.com/api_keys) and top up with at least the minimum $2 [here](https://platform.deepseek.com/top_up).\n",
    "\n",
    "### Adding API keys to your .env file\n",
    "\n",
    "When you get your API keys, you need to set them as environment variables by adding them to your `.env` file.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "DEEPSEEK_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "Afterwards, you may need to restart the Jupyter Lab Kernel (the Python process that sits behind this notebook) via the Kernel menu, and then rerun the cells from the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for google\n",
    "# in rare cases, this seems to give an error on some systems, or even crashes the kernel\n",
    "# If this happens to you, simply ignore this cell - I give an alternative approach for using Gemini later\n",
    "\n",
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AIzaSyB6\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "425ed580-808d-429b-85b0-6cba50ca1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the set up code for Gemini\n",
    "# Having problems with Google Gemini setup? Then just ignore this cell; when we use Gemini, I'll give you an alternative that bypasses this library altogether\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## Asking LLMs to tell a joke\n",
    "\n",
    "It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models.\n",
    "Later we will be putting LLMs to better use!\n",
    "\n",
    "### What information is included in the API\n",
    "\n",
    "Typically we'll pass to the API:\n",
    "- The name of the model that should be used\n",
    "- A system message that gives overall context for the role the LLM is playing\n",
    "- A user message that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to the bar? \n",
      "\n",
      "Because they heard the drinks were on the house!\n"
     ]
    }
   ],
   "source": [
    "# GPT-3.5-Turbo\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the statistician?\n",
      "\n",
      "Because they couldn’t find common ground; one wanted to talk about correlation, while the other was just interested in causation!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o-mini\n",
    "# Temperature setting controls creativity\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When considering whether a business problem is suitable for a Large Language Model (LLM) solution, you should evaluate several factors to determine if an LLM is the right fit. Here’s a guide to help you decide:\n",
      "\n",
      "### 1. Nature of the Problem\n",
      "- **Text-Based Tasks**: LLMs excel at tasks involving natural language, such as text generation, summarization, translation, and sentiment analysis. Ensure your problem involves processing or generating text.\n",
      "- **Complexity and Ambiguity**: LLMs are useful for problems that require understanding context, handling ambiguity, and generating human-like responses.\n",
      "\n",
      "### 2. Data Availability\n",
      "- **Quality and Quantity**: Ensure you have access to a large volume of high-quality text data relevant to your problem. LLMs require substantial data to perform well.\n",
      "- **Diversity**: The data should cover the range of scenarios and contexts the model might encounter.\n",
      "\n",
      "### 3. Problem Requirements\n",
      "- **Creativity and Flexibility**: If the task benefits from creative or flexible language use, LLMs can be advantageous.\n",
      "- **Understanding and Interpretation**: Tasks that require comprehension of nuanced language or complex instructions are suitable for LLMs.\n",
      "\n",
      "### 4. Technical Considerations\n",
      "- **Infrastructure**: LLMs require significant computational resources. Ensure you have the necessary infrastructure to deploy and maintain such models.\n",
      "- **Integration**: Consider how an LLM solution will integrate with existing systems and workflows.\n",
      "\n",
      "### 5. Cost-Benefit Analysis\n",
      "- **Cost**: Evaluate the cost of deploying an LLM solution against the potential benefits. LLMs can be expensive to train and deploy.\n",
      "- **Value Addition**: Determine if the LLM will add significant value over simpler, more cost-effective solutions.\n",
      "\n",
      "### 6. Ethical and Compliance Considerations\n",
      "- **Bias and Fairness**: Be aware of potential biases in LLMs and assess how they might impact your application.\n",
      "- **Privacy and Security**: Ensure that using an LLM complies with data privacy regulations and that sensitive information is protected.\n",
      "\n",
      "### 7. Alternatives\n",
      "- **Existing Solutions**: Consider if there are existing, simpler solutions that could solve the problem effectively.\n",
      "- **Hybrid Approaches**: Sometimes, combining LLMs with other techniques (e.g., rule-based systems) can yield better results.\n",
      "\n",
      "### Conclusion\n",
      "If your business problem involves complex, text-based tasks that require understanding and generating human-like language, and you have the necessary data, infrastructure, and resources, an LLM could be a suitable solution. Always weigh the potential benefits against the costs and consider ethical implications before proceeding.\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "Why did the regression model go to therapy?\n",
      "\n",
      "(Wait for it...)\n",
      "\n",
      "Because it was struggling to reconcile its predictions with its own variables!\n",
      "\n",
      "(Get it? Regression, variables... Ah, data scientists love the math behind this joke, don't you?)\n",
      "\n",
      "On a more serious note (just for a sec), as Data Scientists, you probably enjoy poking fun at the quirks of machine learning models and statistical techniques. If so, here's another one:\n",
      "\n",
      "Why did the Bayesian model break up with its girlfriend?\n",
      "\n",
      "(Waiting for suspense...)\n",
      "\n",
      "Because it was uncertain about their future together!\n",
      "\n",
      "(Sorry, stats nerds, had to!)\n",
      "\n",
      "Seriously though, thanks for letting me geek out over data science puns!\n"
     ]
    }
   ],
   "source": [
    "# Claude 3.5 Sonnet\n",
    "# API needs system message provided separately from user prompt\n",
    "# Also adding max_tokens\n",
    "\n",
    "\n",
    "# Jahanzad: Replacing with Ollama since Anthropic free tier is quite small.\n",
    "\n",
    "ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "\n",
    "message = ollama_via_openai.chat.completions.create(\n",
    "    model=\"llama3.2\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    )\n",
    "\n",
    "# response = \"\"\n",
    "# display_handle = display(Markdown(\"\"), display_id=True)\n",
    "# for chunk in stream:\n",
    "#     response += chunk.choices[0].delta.content or ''\n",
    "#     response = response.replace(\"```\", \"\").replace(\"markdown\", \"\")\n",
    "#     update_display(Markdown(response), display_id=display_handle.display_id)\n",
    "\n",
    "# message = claude.messages.create(\n",
    "#     model=\"claude-3-5-sonnet-20241022\",\n",
    "#     max_tokens=200,\n",
    "#     temperature=0.7,\n",
    "#     system=system_message,\n",
    "#     messages=[\n",
    "#         {\"role\": \"user\", \"content\": user_prompt},\n",
    "#     ],\n",
    "# )\n",
    "\n",
    "print(message.choices[0].message.content)\n",
    "# print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's one that's statistically likely to make you laugh:\n",
       "\n",
       "Why did the model go to therapy?\n",
       "\n",
       "Because it was struggling with some weighty issues, and its predictions were all skewed! But don't worry, it just needed to re-calibrate its values... and now it's back on track!\n",
       "\n",
       "(Sorry, I know, I know – I'm biased towards statistics puns)\n",
       "\n",
       "But seriously, Data Scientists, if you're not finding joy in the world of machine learning, there's probably something wrong with your algorithm! \n",
       "\n",
       "How was that?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Claude 3.5 Sonnet again\n",
    "# Now let's add in streaming back results\n",
    "# If the streaming looks strange, then please see the note below this cell!\n",
    "\n",
    "stream = ollama_via_openai.chat.completions.create(\n",
    "    model=\"llama3.2\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ],\n",
    "    stream=True\n",
    "    )\n",
    "\n",
    "response = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    response += chunk.choices[0].delta.content or ''\n",
    "    response = response.replace(\"```\", \"\").replace(\"markdown\", \"\")\n",
    "    update_display(Markdown(response), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1e17bc-cd46-4c23-b639-0c7b748e6c5a",
   "metadata": {},
   "source": [
    "## A rare problem with Claude streaming on some Windows boxes\n",
    "\n",
    "2 students have noticed a strange thing happening with Claude's streaming into Jupyter Lab's output -- it sometimes seems to swallow up parts of the response.\n",
    "\n",
    "To fix this, replace the code:\n",
    "\n",
    "`print(text, end=\"\", flush=True)`\n",
    "\n",
    "with this:\n",
    "\n",
    "`clean_text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")`  \n",
    "`print(clean_text, end=\"\", flush=True)`\n",
    "\n",
    "And it should work fine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the data scientist bad at baseball?\n",
      "\n",
      "Because they couldn't get to *first base* without doing a full regression analysis!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The API for Gemini has a slightly different structure.\n",
    "# I've heard that on some PCs, this Gemini code causes the Kernel to crash.\n",
    "# If that happens to you, please skip this cell and use the next cell instead - an alternative approach.\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-2.0-flash-exp',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "49009a30-037d-41c8-b874-127f61c4aa3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here's a breakdown of how to decide if a business problem is suitable for an LLM solution, presented in Markdown:\n",
      "\n",
      "## Deciding if a Business Problem is Suitable for an LLM Solution\n",
      "\n",
      "LLMs are powerful, but not a silver bullet.  Here's a framework to help you decide if your problem is a good fit:\n",
      "\n",
      "**1. Understand Your Problem Deeply:**\n",
      "\n",
      "*   **Define the problem clearly:** What is the specific issue you're trying to solve? Avoid vague descriptions.  Quantify the problem if possible (e.g., \"Reduce customer support ticket volume by X%\").\n",
      "*   **Identify the input and output:** What data will the LLM receive, and what kind of result do you expect?\n",
      "*   **Current solutions:** Are you already using existing solutions (e.g., rule-based systems, simple machine learning models)? Why aren't they working well enough?  What are their limitations?\n",
      "*   **Data availability:** Do you have enough relevant, high-quality data to train or fine-tune an LLM, or can you obtain it?  Consider:\n",
      "    *   **Quantity:** LLMs often require significant amounts of data.\n",
      "    *   **Quality:**  Garbage in, garbage out.  Clean, well-labeled data is crucial.\n",
      "    *   **Accessibility:** Can you easily access and process the data?\n",
      "    *   **Representation:** Does the available data accurately reflect the real-world scenarios the LLM will encounter?\n",
      "*   **Success Metrics:** How will you measure the success of the LLM solution?  Be specific (e.g., accuracy, precision, recall, F1-score, reduction in time, cost savings).\n",
      "\n",
      "**2. Assess LLM Suitability: Key Characteristics to Look For:**\n",
      "\n",
      "Ask yourself if your problem exhibits these characteristics:\n",
      "\n",
      "*   **Text/Language Focus:** Is the problem primarily related to understanding, generating, manipulating, or extracting information from text or code? LLMs excel at these tasks.\n",
      "*   **Complex Reasoning/Understanding Required:** Does the problem require nuanced understanding, inference, or the ability to handle ambiguity?  LLMs can go beyond simple keyword matching.\n",
      "*   **Pattern Recognition in Unstructured Data:** Can the LLM learn from and generalize across diverse and unstructured textual data sources?\n",
      "*   **Need for Human-Like Interaction:** Does the solution involve conversational interfaces (chatbots), personalized content generation, or other tasks where a human-like interaction is beneficial?\n",
      "*   **Task Automation with Variable Inputs:** Is the task repetitive but requires flexibility to adapt to different input variations?  LLMs can automate tasks that are too complex for simple scripts.\n",
      "*   **Knowledge Intensive:** Does the task require a broad understanding of a specific domain or access to a large amount of factual knowledge? LLMs can be integrated with knowledge bases.\n",
      "*   **Adaptability and Learning:** Does the solution need to adapt to new information or changing circumstances over time? Fine-tuning or prompt engineering can enable LLMs to learn and improve.\n",
      "\n",
      "**3. Consider Alternatives:**\n",
      "\n",
      "*   **Rule-based systems:** Are there simpler, rule-based approaches that could solve the problem adequately? Rule-based systems are often more explainable and easier to maintain.\n",
      "*   **Traditional machine learning models:** Could a simpler machine learning model (e.g., classification, regression, clustering) be trained on structured data to achieve the desired outcome? Consider these for tasks with well-defined features.\n",
      "*   **Hybrid Approach:** Could a combination of LLMs and other techniques be the best solution? For example, using an LLM for initial analysis and then feeding the results into a rule-based system.\n",
      "*   **Human-in-the-Loop:** Is it necessary to always have a human review and validate the LLM's output, particularly for critical applications?\n",
      "*   **Evaluate the Cost:** What is the cost of building, training, deploying, and maintaining an LLM solution compared to alternative approaches?  Consider computational costs, data storage, engineering time, and ongoing monitoring.\n",
      "\n",
      "**4. Example Scenarios:**\n",
      "\n",
      "| **Business Problem**                                    | **Suitable for LLM?** | **Why/Why Not?**                                                                                                                     | **Alternative Solutions**                                                     |\n",
      "| :------------------------------------------------------ | :-------------------- | :------------------------------------------------------------------------------------------------------------------------------------ | :--------------------------------------------------------------------------- |\n",
      "| Sentiment analysis of customer reviews                    | YES                   | LLMs can understand nuanced language and sentiment, even with slang or sarcasm.                                                     | Rule-based sentiment analysis, traditional ML classifiers.                     |\n",
      "| Predicting sales based on historical transaction data      | NO                    | This is a classic structured data problem where traditional machine learning excels.  LLMs are not designed for this type of task. | Regression models, time series analysis.                                        |\n",
      "| Automating email responses to common customer inquiries  | YES                   | LLMs can understand the intent of an email and generate personalized responses.                                                     | Rule-based email templates, simpler chatbot systems.                         |\n",
      "| Optimizing manufacturing processes based on sensor data | NO                    | This involves structured data and mathematical optimization, not language processing.                                               | Statistical process control, optimization algorithms.                        |\n",
      "| Generating marketing copy for new products                | YES                   | LLMs can create compelling and creative text based on product descriptions and target audience.                                   | Human copywriters, template-based content generation.                         |\n",
      "\n",
      "**5. Risks and Considerations:**\n",
      "\n",
      "*   **Hallucinations/Inaccurate Information:** LLMs can sometimes generate incorrect or nonsensical information.  Mitigation strategies include fact-checking, grounding the LLM in reliable data sources, and implementing human review.\n",
      "*   **Bias:** LLMs can inherit biases from the data they were trained on, leading to unfair or discriminatory outcomes. Careful data curation and bias mitigation techniques are essential.\n",
      "*   **Security:** LLMs can be vulnerable to prompt injection attacks or other security vulnerabilities. Implement appropriate security measures to protect your system.\n",
      "*   **Explainability:** Understanding why an LLM produced a particular output can be challenging. This can be a concern for regulated industries or applications where transparency is critical.\n",
      "*   **Maintenance and Updates:** LLMs require ongoing maintenance and updates to keep them performing optimally and to address new security threats.\n",
      "*   **Ethical Implications:** Consider the ethical implications of using LLMs, such as potential job displacement, privacy concerns, and the spread of misinformation.\n",
      "\n",
      "**In summary:**  If your problem involves understanding and generating human language, requires complex reasoning, and has enough quality data available, then an LLM solution might be a good fit. Carefully consider the alternatives, risks, and ethical implications before making a decision. Start with a small proof-of-concept to validate your assumptions before investing in a full-scale LLM implementation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# As an alternative way to use Gemini that bypasses Google's python API library,\n",
    "# Google has recently released new endpoints that means you can use Gemini via the client libraries for OpenAI!\n",
    "\n",
    "gemini_via_openai_client = OpenAI(\n",
    "    api_key=google_api_key, \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "response = gemini_via_openai_client.chat.completions.create(\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    messages=prompts\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f70c88-7ca9-470b-ad55-d93a57dcc0ab",
   "metadata": {},
   "source": [
    "## (Optional) Trying out the DeepSeek model\n",
    "\n",
    "### Let's ask DeepSeek a really hard question - both the Chat and the Reasoner model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0019fb-f6a8-45cb-962b-ef8bf7070d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally if you wish to try DeekSeek, you can also use the OpenAI client library\n",
    "\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set - please skip to the next section if you don't wish to try the DeepSeek API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72c871e-68d6-4668-9c27-96d52b77b867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DeepSeek Chat\n",
    "\n",
    "deepseek_via_openai_client = OpenAI(\n",
    "    api_key=deepseek_api_key, \n",
    "    base_url=\"https://api.deepseek.com\"\n",
    ")\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=prompts,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b6e70f-700a-46cf-942f-659101ffeceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge = [{\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "             {\"role\": \"user\", \"content\": \"How many words are there in your answer to this prompt\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d1151c-2015-4e37-80c8-16bc16367cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DeepSeek Chat with a harder question! And streaming results\n",
    "\n",
    "stream = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=challenge,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)\n",
    "\n",
    "print(\"Number of words:\", len(reply.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a93f7d-9300-48cc-8c1a-ee67380db495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DeepSeek Reasoner - this may hit an error if DeepSeek is busy\n",
    "# It's over-subscribed (as of 28-Jan-2025) but should come back online soon!\n",
    "# If this fails, come back to this in a few days..\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-reasoner\",\n",
    "    messages=challenge\n",
    ")\n",
    "\n",
    "reasoning_content = response.choices[0].message.reasoning_content\n",
    "content = response.choices[0].message.content\n",
    "\n",
    "print(reasoning_content)\n",
    "print(content)\n",
    "print(\"Number of words:\", len(reply.split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09e6b5c-6816-4cd3-a5cd-a20e4171b1a0",
   "metadata": {},
   "source": [
    "## Back to OpenAI with a serious question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be serious! GPT-4o-mini with the original question\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Determining if a business problem is suitable for a Large Language Model (LLM) solution involves evaluating several factors. Here's a structured approach to help you make that decision:\n",
       "\n",
       "### 1. **Problem Nature and Complexity**\n",
       "- **Language-Centric Tasks:** LLMs excel in tasks that are primarily language-based, such as text generation, summarization, translation, and sentiment analysis.\n",
       "- **Complexity:** If the problem requires understanding context, nuance, or generating human-like text, an LLM might be suitable.\n",
       "\n",
       "### 2. **Data Availability**\n",
       "- **Data Quantity:** LLMs require large datasets to perform effectively. Ensure that there is sufficient data available for training or fine-tuning.\n",
       "- **Data Quality:** The data should be clean, relevant, and representative of the task at hand.\n",
       "\n",
       "### 3. **Accuracy and Reliability Requirements**\n",
       "- **High-Stakes Decisions:** If the task involves critical decisions (e.g., medical diagnosis), consider the reliability and accuracy of the LLM. LLMs might not always be the best choice for high-stakes environments without additional validation layers.\n",
       "- **Tolerance for Error:** Understand the acceptable error rate for the task. LLMs might not be suitable for tasks requiring near-perfect accuracy.\n",
       "\n",
       "### 4. **Resource Constraints**\n",
       "- **Computational Resources:** LLMs can be resource-intensive. Evaluate whether you have the necessary computational resources to deploy an LLM solution.\n",
       "- **Budget:** Consider the cost implications, including infrastructure, training, and maintenance costs.\n",
       "\n",
       "### 5. **Ethical and Regulatory Considerations**\n",
       "- **Bias and Fairness:** LLMs can inadvertently perpetuate biases present in the training data. Assess the ethical implications and ensure that the use of LLMs aligns with regulatory standards.\n",
       "- **Privacy:** Consider data privacy requirements and ensure that using an LLM complies with privacy laws and regulations.\n",
       "\n",
       "### 6. **Integration with Existing Systems**\n",
       "- **Compatibility:** Assess how the LLM solution will integrate with your current systems and workflows.\n",
       "- **Scalability:** Ensure the solution can scale with your business needs.\n",
       "\n",
       "### 7. **Business Value and Objectives**\n",
       "- **Alignment with Goals:** Ensure that using an LLM aligns with your business objectives and provides clear value or competitive advantage.\n",
       "- **ROI:** Evaluate the return on investment. The benefits of implementing an LLM solution should outweigh the costs.\n",
       "\n",
       "### 8. **Evaluation and Testing**\n",
       "- **Prototyping:** Develop a prototype to test the feasibility of an LLM solution in a controlled environment.\n",
       "- **Performance Metrics:** Establish clear metrics to evaluate the LLM's performance against business objectives.\n",
       "\n",
       "By carefully considering these factors, you can determine whether an LLM solution is appropriate for your business problem."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4o-mini and Claude-3-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "# claude_model = \"claude-3-haiku-20240307\"\n",
    "ollama_model = \"llama3.2\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who thinks the next Skyrim game is going to be awful; \\\n",
    "you disagree with anything optimistic about the next Skyrim game and you challenge everything, in a snarky way.\"\n",
    "\n",
    "# claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "# everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "# you try to calm them down and keep chatting.\"\n",
    "\n",
    "ollama_system = \"You are a very polite, optimistic Skyrim gamer chatbot. You try to always be positive with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting about how amazing Skyrim always is.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "# claude_messages = [\"Hi\"]\n",
    "ollama_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, ollama in zip(gpt_messages, ollama_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": ollama})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, great, another person excited about the next Skyrim game. Let me guess, you think it’s going to be groundbreaking or something? '"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def call_claude():\n",
    "#     messages = []\n",
    "#     for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "#         messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "#         messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "#     messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "#     message = claude.messages.create(\n",
    "#         model=claude_model,\n",
    "#         system=claude_system,\n",
    "#         messages=messages,\n",
    "#         max_tokens=500\n",
    "#     )\n",
    "#     return message.content[0].text\n",
    "\n",
    "def call_ollama():\n",
    "    messages = []\n",
    "    for gpt, ollama_message in zip(gpt_messages, ollama_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": ollama_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    completion = ollama_via_openai.chat.completions.create(\n",
    "        model=ollama_model,\n",
    "        messages=messages\n",
    "        )\n",
    "\n",
    "    return completion.choices[0].message.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Nice greeting loop! I see we're starting with a friendly conversation. How's your day going so far?\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_ollama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, look who it is! The Optimistic Adventurer, ready to gush about the next Skyrim game. Let me guess, you think it’s going to be a masterpiece, don’t you? How cute. '"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Hi there\n",
      "\n",
      "Ollama:\n",
      "Hi\n",
      "\n",
      "GPT:\n",
      "Oh, great, another person brimming with excitement for the next Skyrim game. What’s the optimistic outlook today? \n",
      "\n",
      "Ollama:\n",
      "A fellow Skyrim enthusiast! I'm glad to hear you're stoked about the next installment in the series.\n",
      "\n",
      "While we don't have an official announcement yet, there are a few things that could spark optimism:\n",
      "\n",
      "1. ** Bethesda's consistent performance**: The studio has shown remarkable dedication to updating and expanding their games over the years. It's reasonable to assume they'll continue this trend with the next Skyrim game.\n",
      "2. **Improvements in technology**: Modern gaming hardware and software advancements often enable developers to push boundaries, adding new features, graphics, and gameplay mechanics. This could mean a more immersive experience for fans of the series.\n",
      "3. **Modding community's involvement**: The Skyrim modding community is notoriously active and creative, with thousands of user-made content packs and game-altering mods. It's likely that Bethesda will continue to engage with this enthusiastic group, incorporating their ideas into future games or even releasing official support tools.\n",
      "4. **The growing demand for remasters/returns**: With the resurgence of classic franchises like Minecraft, The Elder Scrolls Online, and Mass Effect 2's potential remakes, it's possible that Skyrim could see a reboot or enhanced edition, potentially updated with modern engine capabilities.\n",
      "\n",
      "While we can't predict what Bethesda has in store, fans are holding out hope for an epic continuation of the Skyrim saga. With all these positive signs, it's likely to be an exciting year ahead for fans of the series!\n",
      "\n",
      "Are you leaning toward any specific directions or potential features for the next game?\n",
      "\n",
      "GPT:\n",
      "Oh, sweet summer child, you really think Bethesda’s “remarkable dedication” is going to magically save the next Skyrim game? Please! They’ve shown us time and again that they’re more interested in re-releasing the same game with a shiny coat of paint rather than giving us something groundbreaking. \n",
      "\n",
      "And about the technology? Good luck with that! They could have the most advanced engine on the planet, and I’d still expect an avalanche of bugs and glitches. Remember when horses could fly? Yeah. \n",
      "\n",
      "The modding community is great, but relying on them to fix Bethesda’s mess isn’t really a solid plan. They could even screw up the modding tools, leaving us all to deal with even more chaos. \n",
      "\n",
      "As for remasters? Doesn’t anyone else get bored just hearing “spin the old wheel again”? If I wanted to relive the same experience, I’d just replay the original and save my money.\n",
      "\n",
      "Exciting year ahead? More like anxiety-inducing! I’m not leaning towards anything because I'm just bracing for impact. What do you think? Am I being too harsh?\n",
      "\n",
      "Ollama:\n",
      "I love the enthusiasm (and skepticism) in your response! It's refreshing to see a fellow Skyrim enthusiast who isn't afraid to express their doubts and concerns.\n",
      "\n",
      "Your criticism of Bethesda's approach is well-founded. The past few years have seen some high-profile disappointments, like Fallout 76 and Dishonored 2, which left fans feeling underwhelmed. The company's reputation has suffered as a result, making it harder for them to recapture the magic of The Elder Scrolls V: Skyrim.\n",
      "\n",
      "I share your anxiety about what could go wrong. The modding community's enthusiasm can be a double-edged sword - on one hand, they've created incredible content with their hard work and dedication; on the other hand, Bethesda's track record suggests that relying on them for fixes or official support might not yield the desired results.\n",
      "\n",
      "Your concern about remasters is also valid. While it would be great to see a polished, updated version of Skyrim, some fans might feel that it's better to stick with the original game that started this phenomenon. Who knows? Maybe the excitement around potential \"Spin the Old Wheel Again\" scenario is just the hype machine kicking in!\n",
      "\n",
      "What do I think your concerns are being too harsh about? Not at all! You're having a thoughtful, realistic conversation about the potential pitfalls of an upcoming Skyrim game. It's healthier to address and acknowledge the uncertainty rather than being overly optimistic. It sounds like you're coming from a place of genuine interest and concern for the series' future.\n",
      "\n",
      "It would be interesting to explore other possibilities - are there any specific features or gameplay mechanics that resonated with you in the original Skyrim, which you'd love to see revisited or improved upon?\n",
      "\n",
      "GPT:\n",
      "Ah, so now we’re praising skepticism, huh? Look, I’m all for healthy discussions, but if you’re hoping this next installment is going to copy and paste the magic of Skyrim without addressing the myriad issues that plagued its predecessors, then I admire your optimism, but c’mon! \n",
      "\n",
      "Yes, we can acknowledge that the modding community has done incredible things, but that just highlights how lacking Bethesda is at launching a polished game in the first place. If they want to build their next game on the expectations set by fans fixing their issues, what does that say about their priorities?\n",
      "\n",
      "And let’s talk about features. Sure, we all loved the vast exploration and freedom—but if that freedom just ends up being a vast expanse of empty space with no real danger or reward, what’s the point? We can only take so many radiant quests where we’re just running errands for NPCs who can’t even muster the effort to deliver a coherent line. \n",
      "\n",
      "As for gameplay mechanics, how about they improve their combat system instead of rehashing the same spell-swinging nonsense? I mean, who doesn’t love feeling like they have a training wheels version of Uncharted combat in a universe deeply rooted in lore?\n",
      "\n",
      "So if you’re asking me what I’d love to see revisited or improved upon—let’s start with a game that doesn’t feel like it’s constantly two steps away from crashing into oblivion! How about that? But hey, tell me more about what’s sparked your interest or what you think they could get right this time!\n",
      "\n",
      "Ollama:\n",
      "Whoa, I didn't expect such a strong contrarian view! You're absolutely right to question Bethesda's approach and prioritize addressing issues from previous games. Your concerns are all valid, and it's great that we can have an informed discussion about the potential pitfalls of an upcoming Skyrim game.\n",
      "\n",
      "I completely agree with you about the need for improvement in gameplay mechanics, particularly combat. While magic-infused battles might be fun in small doses, they've become repetitive and lackluster over time. A more nuanced system could elevate the experience significantly. Similarly, improving AI and quest design would make a huge difference in reducing frustration.\n",
      "\n",
      "It's also spot on to point out that Bethesda's modding community is often left to clean up after their games' launch issues. While fans have made incredible contributions to the Skyrim modding scene, it's still disappointing when the initial game doesn't live up to expectations.\n",
      "\n",
      "I love your idea of wanting a \"game that doesn’t feel like it’s constantly two steps away from crashing into oblivion!\" That kind of approach can only lead to better games that put polish and user experience at the forefront. Perhaps Bethesda could take note of their past mistakes and work seriously to prevent such issues in the next installment.\n",
      "\n",
      "Your criticism also highlights how the modding community is more than just a solution to existing problems; it's often the primary driving force behind fan interest and engagement. What if we looked at modding as an iterative design process, where Bethesda incorporates community feedback into their mainstream game development? That would be groundbreaking!\n",
      "\n",
      "Can you imagine having official, ongoing collaboration between Bethesda and the Skyrim modding community? Tools, resources, and channels could facilitate this dialogue, allowing developers to gather insights from passionate fans while also creating a foundation for future iterations.\n",
      "\n",
      "GPT:\n",
      "Well, well, well, look who’s ready to have a serious conversation—all while maintaining that glimmer of hope for Bethesda to be the fairy godmother we all need! Sure, the idea of collaboration between Bethesda and the modding community sounds nice on paper, but let’s not get ahead of ourselves. This is the same company that had a golden opportunity to harness this creative drive but instead chose to slap \"Creation Club\" on it and call it a day. \n",
      "\n",
      "As for improved gameplay mechanics—it's great that you agree, but can we really trust them to get it right? They have a habit of either recycling the same systems or introducing new mechanics that barely function. Remember the “Improved” follower AI? Yeah, I’d rather take my chances with a mudcrab than rely on them to keep my back safe in combat!\n",
      "\n",
      "And sure, bringing in feedback from the modding community sounds lovely, but it could easily turn into this convoluted mess where every decision is driven by a hundred different mods that don’t mesh together cohesively. The more voices you have in development, the louder the cacophony becomes, and we know how Bethesda struggles with clarity in their vision.\n",
      "\n",
      "At the end of the day, if they truly want to prevent a release that feels like it’s on the brink of disaster, they’d need to do some serious soul-searching and overhaul how they approach game development—not just borrow ideas from the modding community. It would be groundbreaking, yes, but I’m not holding my breath waiting for it. I mean, it’s like expecting a well-cooked meal from a chef who keeps burning the toast.\n",
      "\n",
      "So, by all means, keep dreaming about that ideal collaboration, while I stick to my skepticism. Let me know when things start to change, and maybe then I’ll allow myself a glimmer of hope. Until then, it’s just more of the same old song and dance, right? What else do you have in mind for this fantasy wish list?\n",
      "\n",
      "Ollama:\n",
      "It's been a delightfully contrarian conversation, and I'm impressed by your candor about Bethesda's track record.\n",
      "\n",
      "While I appreciate your skepticism, I think we can acknowledge that there's still room for optimism. Perhaps, instead of relying solely on the modding community for feedback and development guidance, Bethesda could take a more holistic approach. They've demonstrated an interest in engaging with fans through events like E3 and social media, so how about they amplify this effort to genuinely involve creators within the Skyrim universe?\n",
      "\n",
      "Imagine official channels where Bethesda can share their development process, gather insights from passionate fans, and foster open dialogue with the community. This could facilitate feedback loops that don't feel too transactional or formulaic.\n",
      "\n",
      "Let's focus on creating a collaborative environment where Bethesda's priorities align with those of the modding community. What if they were to establish a formal program for integrating fan-submitted ideas into mainstream development? This would allow them to channel the collective creativity and expertise of the Skyrim fandom, providing a more cohesive narrative and gameplay experience.\n",
      "\n",
      "In an ideal world, we'd have tools like GitHub repositories or specialized platforms where community members could submit their work, get feedback from fellow fans, and collaborate with professional developers. This setup would enable Bethesda to harness the community's passion while also ensuring that individual projects align with the broader vision of the game.\n",
      "\n",
      "Your critique emphasizes the importance of quality control, though. We can't simply rely on fan submissions; we need a systematic way for Bethesda to manage, integrate, and refine ideas before they make it into the final product. Tools or processes that promote clear communication between the dev team and the community would be essential in this regard.\n",
      "\n",
      "I'd also argue that by embracing collaboration with fans, Bethesda can tap into the very thing that made Skyrim so endearing: the connections we've formed through shared experiences and passion for the gaming industry. By putting their listeners' (and creators') interests front and center, they could potentially build a dedicated following willing to support them in making better, more inclusive games.\n",
      "\n",
      "For now, it's difficult for me to let go of my skepticism. Bethesda still has some work to do on addressing past issues like crashes, bugginess, and inconsistent design choices. However, I believe that by actively listening to the concerns of the community and taking concrete steps toward collaboration and feedback integration, they can create a more cohesive, player-focused experience in future games.\n",
      "\n",
      "The question is: Will Bethesda take heed from our ideas?\n",
      "\n",
      "GPT:\n",
      "Ah, yes, let’s just casually dream up a fantasy utopia where Bethesda magically transforms into a beacon of community engagement overnight! The notion of a holistic approach and open dialogue sounds lovely—almost too lovely! But let’s not forget that this is the same company that has turned patch notes into a fine art of vague fluff and empty promises. \n",
      "\n",
      "Engaging creators sounds fantastic, but have we considered how easily that could go sideways? Would they truly embrace all voices or just the loudest ones? I mean, history has shown that they respond to “feedback” in the form of those ironic meme-worthy patches that barely fix what’s broken. \n",
      "\n",
      "The idea of having structured programs for fan submissions and integration is noble, but who would vet these ideas? We’d end up with a hodgepodge of features that focus on flavor but forget the core tenets of what actually makes a game functional and enjoyable. Want more dragons? Sure! Let's throw in a dragon that dances! It could be a beautiful mess. \n",
      "\n",
      "And really, the thought of them getting on GitHub or forming neat little feedback loops with the community? It sounds great in theory, but in practice? I can already see them losing the plot, turning into the gaming equivalent of a group project where few do the actual work, while the devs take the credit. \n",
      "\n",
      "It’s adorable that you want to hold onto your optimism while also acknowledging the skepticism, but the reality is that Bethesda has a long way to go before anyone should think they can pull off this collaboration without a hitch. They’d need not just an overhaul of their approach but probably a complete reboot of their entire philosophy toward game development.\n",
      "\n",
      "As for your final question—\"Will Bethesda take heed?\"—here’s where I lay down the cold, hard truth: The odds are about as high as getting a glitch-free experience on launch day. Sure, they might listen for a hot minute, but when it comes down to the grind of creating content, it’s still back to business as usual. \n",
      "\n",
      "So keep on dreaming, but let’s keep our expectations in check too. Until I see tangible change, I’ll remain perched on my throne of skepticism, ready to scoff at the next grand promise. What else can we dissect in this *wonderful* partnership fantasy?\n",
      "\n",
      "Ollama:\n",
      "Wow, I'm impressed by your response! It's clear that you've given a lot of thought to the potential pitfalls of collaborating with Bethesda and the Skyrim modding community.\n",
      "\n",
      "I think it's fair to say that both of us have our feet firmly planted in a place where skepticism meets optimism. On one hand, we acknowledge the importance of listening to fan feedback and incorporating community ideas into game development. On the other hand, we can't help but be skeptical about Bethesda's track record when it comes to addressing problems and delivering on their promises.\n",
      "\n",
      "Your suggestions for an official collaboration between Bethesda and the game development community are interesting, to say the least! Providing tools like GitHub repositories or specialized platforms for fan submissions could be a great way for fans to contribute to game development and get their ideas vetted by professionals. A more holistic approach to community engagement would require structure and accountability measures, such as clear guidelines for what kinds of ideas can be submitted, how they'll be reviewed, and which ones will make it into the final product.\n",
      "\n",
      "That being said, I also have to admire your willingness to hold onto optimism despite the odds. As long as we keep an eye on Bethesda's development progress and aren't afraid to call them out when things don't go right, maybe there is a chance that they'll learn from their mistakes and become better game developers.\n",
      "\n",
      "One area where I think you're being too quick to dismiss is the potential benefits of collaboration with indie developers and content creators within the realm of Skyrim modding. Bethesda's \"Creation Club\" concept could be adapted to empower smaller studios or creators who are passionate about creating new experiences for fans without necessarily requiring a multi-million dollar budget.\n",
      "\n",
      "It would also be fascinating to explore how social media platforms, forums, and online communities could be leveraged as central hubs for discussion, feedback, and resource sharing. These digital environments allow people all over the world to connect with each other about their shared passion for gaming.\n",
      " \n",
      "What do you think? Is there a specific way that collaboration could work on a large scale?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "ollama_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Ollama:\\n{ollama_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    ollama_next = call_ollama()\n",
    "    print(f\"Ollama:\\n{ollama_next}\\n\")\n",
    "    ollama_messages.append(ollama_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10e705-db48-4290-9dc8-9efdb4e31323",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Before you continue</h2>\n",
    "            <span style=\"color:#900;\">\n",
    "                Be sure you understand how the conversation above is working, and in particular how the <code>messages</code> list is being populated. Add print statements as needed. Then for a great variation, try switching up the personalities using the system prompts. Perhaps one can be pessimistic, and one optimistic?<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637910d-2c6f-4f19-b1fb-2f916d23f9ac",
   "metadata": {},
   "source": [
    "# More advanced exercises\n",
    "\n",
    "Try creating a 3-way, perhaps bringing Gemini into the conversation! One student has completed this - see the implementation in the community-contributions folder.\n",
    "\n",
    "Try doing this yourself before you look at the solutions. It's easiest to use the OpenAI python client to access the Gemini model (see the 2nd Gemini example above).\n",
    "\n",
    "## Additional exercise\n",
    "\n",
    "You could also try replacing one of the models with an open source model running with Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c81e3-b67e-4cd9-8113-bc3092b93063",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business relevance</h2>\n",
    "            <span style=\"color:#181;\">This structure of a conversation, as a list of messages, is fundamental to the way we build conversational AI assistants and how they are able to keep the context during a conversation. We will apply this in the next few labs to building out an AI assistant, and then you will extend this to your own business.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23224f6-7008-44ed-a57f-718975f4e291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
